{
  "title": "String.fromCodePoint",
  "description": "Create a string from Unicode code points.",
  "code": "// In the days before Unicode, we had 2^8 = 256\n// characters and, then, 2^16 = 65,536 characters\n// in our character sets. We could represent each\n// of those characters as a number, or code, for\n// the specific character encoding we used in\n// our software. Pre-Unicode ECMAscript provides\n// the String.fromCharCode method to build strings\n// from those character codes.\n\n// This converts the code 72 to \"H\" and 105 to \"i\"\n// and concatenates them to form \"Hi\".\nconsole.log(String.fromCharCode(72, 105));\n\n// But, because we had a limit of 65,536\n// characters, the String.fromCharCode method\n// would modulo the argument to prevent those\n// pesky exceptions that early Web \"programmers\"\n// hated.\nconsole.log(String.fromCharCode(72 + 65536, 105));\n\n// Then, Unicode came along with its\n// Supplementary Multilingual Plane and its\n// Supplementary Ideographic Plane and more,\n// characters represented by more than one\n// character code which Unicode calls a \"code\n// point\". Now, String.fromCharCode doesn't work.\nconsole.log(String.fromCharCode(127776));\n\n// String.fromCodePoint exists to resolve the\n// values outside the Basic Multilingual Plane.\nconsole.log(String.fromCodePoint(127776));"
}
